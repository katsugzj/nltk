# nltk_homework
## 第一次实验(预处理)
### 1.由DTD文件可知一个DOC标签下存在多个TEXT标签的情况
### 2.通过正则表达式读取标签的内容
### 3.首先词条化，并去除停用词和非字母和非数字，并对结果词性归并，对每个DOCNO输出一个文件保存结果
### 4.只保存了TEXT的内容，文件名为DOCNO
### 5.总运行时间为4.5小时
### 6.处理完成的文件共有741724个
## 第二次实验(倒排索引)
### 1.使用FreqDist方法直接统计词频，将结果以{词条：{DOCNO：词频}}的方式存储
### 2.总运行时间为40分钟
### 3.未使用归并，python的内存占用最高到达3.5G
### 4.处理后的文件有1.74G，文本编辑器中记事本，sublime和VScode都打不开，notepad++勉强可以(会有几秒卡顿)
## 第三次实验(检索模型)
### 1.构建索引时未统计总词数，所以使用文档大小来近似
### 2.没有构建布尔索引，仅使用了tfidf和bm25对文档打分
### 3.可以对topics的每个单词使用tfidf进行打分，然后对每个部分的title和两个描述总的单个单词的打分最高的几个单词作为检索关键字，但是处理量较大，只简单的使用title进行检索
### 4.第一实验处理后的文档较多，导致匹配文档来获取长度的时间过长，因此使用bm25处理时间比起简单使用tfidf的时长要多出许多
### 5.从实验结果来看，单纯使用tfidf得到的结果非常差，不论是准确率还是召回率都远远低于bm25所得到的结果